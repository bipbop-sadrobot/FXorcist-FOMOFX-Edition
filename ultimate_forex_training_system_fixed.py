#!/usr/bin/env python3
"""
Ultimate Forex Training System with Lorentzian Classification
===============================================================================
Comprehensive Python script for advanced Forex trading training incorporating:

üî¨ ADVANCED INDICATORS (Priority):
‚îú‚îÄ‚îÄ Lorentzian Classification (ML-based pattern recognition)
‚îú‚îÄ‚îÄ Fractal Analysis
‚îú‚îÄ‚îÄ ZigZag Indicator
‚îú‚îÄ‚îÄ Andrews Pitchfork
‚îú‚îÄ‚îÄ Gann Angles/Fans
‚îú‚îÄ‚îÄ Fibonacci Extensions/Retracements
‚îú‚îÄ‚îÄ Harmonic Patterns
‚îú‚îÄ‚îÄ Elliott Wave Analysis

üìä TECHNICAL INDICATORS:
‚îú‚îÄ‚îÄ RSI (Relative Strength Index)
‚îú‚îÄ‚îÄ MACD (Moving Average Convergence Divergence)
‚îú‚îÄ‚îÄ Bollinger Bands
‚îú‚îÄ‚îÄ Stochastic Oscillator
‚îú‚îÄ‚îÄ Williams %R
‚îú‚îÄ‚îÄ CCI (Commodity Channel Index)
‚îú‚îÄ‚îÄ ATR (Average True Range)
‚îú‚îÄ‚îÄ ADX (Average Directional Index)
‚îú‚îÄ‚îÄ Ichimoku Cloud
‚îú‚îÄ‚îÄ Aroon Oscillator (Fixed Implementation)
‚îú‚îÄ‚îÄ Vortex Indicator
‚îú‚îÄ‚îÄ Keltner Channels

üí∞ VOLUME ANALYSIS:
‚îú‚îÄ‚îÄ OBV (On Balance Volume)
‚îú‚îÄ‚îÄ Chaikin Money Flow
‚îú‚îÄ‚îÄ Force Index
‚îú‚îÄ‚îÄ Volume Profile
‚îú‚îÄ‚îÄ Volume Weighted Average Price (VWAP)
‚îú‚îÄ‚îÄ Accumulation/Distribution Line
‚îú‚îÄ‚îÄ Money Flow Index (MFI)
‚îú‚îÄ‚îÄ Ease of Movement (EMV)

üéØ MACHINE LEARNING:
‚îú‚îÄ‚îÄ CatBoost Regression for Price Prediction
‚îú‚îÄ‚îÄ Feature Importance Analysis
‚îú‚îÄ‚îÄ Model Evaluation Metrics
‚îú‚îÄ‚îÄ Cross-Validation
‚îú‚îÄ‚îÄ Hyperparameter Optimization

üìà VISUALIZATION & ANALYSIS:
‚îú‚îÄ‚îÄ Price Charts with Indicators
‚îú‚îÄ‚îÄ Feature Correlation Analysis
‚îú‚îÄ‚îÄ Model Performance Plots
‚îú‚îÄ‚îÄ Prediction vs Actual Analysis
‚îú‚îÄ‚îÄ Risk/Reward Analysis

üèóÔ∏è PRODUCTION FEATURES:
‚îú‚îÄ‚îÄ Modular Architecture
‚îú‚îÄ‚îÄ Comprehensive Error Handling
‚îú‚îÄ‚îÄ Detailed Logging
‚îú‚îÄ‚îÄ Model Persistence
‚îú‚îÄ‚îÄ Scalable Data Processing
‚îú‚îÄ‚îÄ Memory Optimization
‚îú‚îÄ‚îÄ Real-time Prediction Capability

Author: Kilo Code
Version: 2.0.0
Date: 2025-09-03
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import logging
import warnings
import json
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Optional, Tuple, Union
import multiprocessing as mp
from functools import wraps
import gc
import psutil
import os

# Machine Learning
from catboost import CatBoostRegressor, Pool
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler, RobustScaler
from scipy import stats
from scipy.spatial.distance import pdist, squareform

warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/ultimate_forex_training.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Constants
MEMORY_THRESHOLD = 80  # Memory usage threshold (%)
MAX_WORKERS = min(mp.cpu_count(), 8)
RANDOM_SEED = 42

class MemoryMonitor:
    """Monitor memory usage and trigger cleanup when needed"""

    @staticmethod
    def get_memory_usage():
        """Get current memory usage percentage"""
        return psutil.virtual_memory().percent

    @staticmethod
    def should_cleanup():
        """Check if memory cleanup is needed"""
        return MemoryMonitor.get_memory_usage() > MEMORY_THRESHOLD

    @staticmethod
    def cleanup():
        """Perform memory cleanup"""
        gc.collect()
        memory_usage = MemoryMonitor.get_memory_usage()
        logger.info(".1f")

class LorentzianClassifier:
    """
    Lorentzian Classification - Advanced ML-based Pattern Recognition
    Uses Lorentzian distance metrics for sophisticated market classification
    """

    def __init__(self, lookback_periods: List[int] = [10, 20, 30, 50, 100]):
        self.lookback_periods = lookback_periods
        self.classifiers = {}
        self.scalers = {}

    def lorentzian_distance(self, x: np.ndarray, y: np.ndarray) -> float:
        """Calculate Lorentzian distance between two vectors"""
        diff = np.abs(x - y)
        return np.sum(np.log(1 + diff))

    def fit(self, X: pd.DataFrame, y: pd.Series, symbol: str = 'default'):
        """Train Lorentzian classifiers for different lookback periods"""
        logger.info(f"Training Lorentzian Classification for {symbol}")

        for period in self.lookback_periods:
            logger.info(f"Training Lorentzian classifier for {period}-period lookback")

            # Create rolling windows
            X_windows = []
            y_labels = []

            for i in range(period, len(X)):
                window = X.iloc[i-period:i].values.flatten()
                label = y.iloc[i]
                X_windows.append(window)
                y_labels.append(label)

            X_windows = np.array(X_windows)
            y_labels = np.array(y_labels)

            # Scale features
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_windows)

            # Store training data for distance-based classification
            self.classifiers[f"{symbol}_{period}"] = {
                'X_train': X_scaled,
                'y_train': y_labels,
                'scaler': scaler
            }

            self.scalers[f"{symbol}_{period}"] = scaler

        logger.info(f"‚úÖ Lorentzian Classification trained for {len(self.lookback_periods)} periods")

    def predict(self, X: pd.DataFrame, symbol: str = 'default') -> Dict[str, np.ndarray]:
        """Make predictions using Lorentzian distance-based classification"""
        predictions = {}

        for period in self.lookback_periods:
            key = f"{symbol}_{period}"

            if key not in self.classifiers:
                logger.warning(f"No trained classifier for {key}")
                continue

            # Get training data
            X_train = self.classifiers[key]['X_train']
            y_train = self.classifiers[key]['y_train']
            scaler = self.classifiers[key]['scaler']

            # Create prediction windows
            pred_windows = []
            for i in range(period, len(X)):
                window = X.iloc[i-period:i].values.flatten()
                pred_windows.append(window)

            if not pred_windows:
                logger.warning(f"Insufficient data for {period}-period prediction")
                continue

            X_pred = np.array(pred_windows)
            X_pred_scaled = scaler.transform(X_pred)

            # Calculate Lorentzian distances and make predictions
            pred_labels = []
            for x_pred in X_pred_scaled:
                distances = np.array([self.lorentzian_distance(x_pred, x_train)
                                    for x_train in X_train])
                # Find k nearest neighbors (k=5)
                k = min(5, len(distances))
                nearest_indices = np.argsort(distances)[:k]
                nearest_labels = y_train[nearest_indices]

                # Weighted prediction based on distance
                weights = 1 / (distances[nearest_indices] + 1e-6)  # Avoid division by zero
                prediction = np.average(nearest_labels, weights=weights)
                pred_labels.append(prediction)

            predictions[f"lorentzian_{period}"] = np.array(pred_labels)

        return predictions

class AdvancedTechnicalIndicators:
    """Advanced Technical Indicators Implementation"""

    @staticmethod
    def calculate_fractals(high: pd.Series, low: pd.Series, period: int = 5) -> Tuple[pd.Series, pd.Series]:
        """Calculate Fractal indicators (Williams Fractals)"""
        # Bearish fractal: high is higher than period-1 previous and period-1 next highs
        bearish_fractal = pd.Series(index=high.index, dtype=float)

        # Bullish fractal: low is lower than period-1 previous and period-1 next lows
        bullish_fractal = pd.Series(index=low.index, dtype=float)

        for i in range(period, len(high) - period):
            # Bearish fractal
            if all(high.iloc[i] > high.iloc[i-j] for j in range(1, period+1)) and \
               all(high.iloc[i] > high.iloc[i+j] for j in range(1, period+1)):
                bearish_fractal.iloc[i] = high.iloc[i]

            # Bullish fractal
            if all(low.iloc[i] < low.iloc[i-j] for j in range(1, period+1)) and \
               all(low.iloc[i] < low.iloc[i+j] for j in range(1, period+1)):
                bullish_fractal.iloc[i] = low.iloc[i]

        return bearish_fractal, bullish_fractal

    @staticmethod
    def calculate_zigzag(high: pd.Series, low: pd.Series, percentage: float = 5.0) -> pd.Series:
        """Calculate ZigZag indicator"""
        zigzag = pd.Series(index=high.index, dtype=float)
        direction = 0  # 0 = undefined, 1 = up, -1 = down
        last_pivot = 0

        for i in range(1, len(high)):
            if direction == 0:
                # Look for first significant move
                if high.iloc[i] >= high.iloc[last_pivot] * (1 + percentage/100):
                    direction = 1
                    zigzag.iloc[i] = high.iloc[i]
                    last_pivot = i
                elif low.iloc[i] <= low.iloc[last_pivot] * (1 - percentage/100):
                    direction = -1
                    zigzag.iloc[i] = low.iloc[i]
                    last_pivot = i
            elif direction == 1:
                # Looking for downward move
                if low.iloc[i] <= high.iloc[last_pivot] * (1 - percentage/100):
                    direction = -1
                    zigzag.iloc[i] = low.iloc[i]
                    last_pivot = i
                elif high.iloc[i] > high.iloc[last_pivot]:
                    # Update pivot if higher high
                    zigzag.iloc[last_pivot] = np.nan
                    zigzag.iloc[i] = high.iloc[i]
                    last_pivot = i
            else:  # direction == -1
                # Looking for upward move
                if high.iloc[i] >= low.iloc[last_pivot] * (1 + percentage/100):
                    direction = 1
                    zigzag.iloc[i] = high.iloc[i]
                    last_pivot = i
                elif low.iloc[i] < low.iloc[last_pivot]:
                    # Update pivot if lower low
                    zigzag.iloc[last_pivot] = np.nan
                    zigzag.iloc[i] = low.iloc[i]
                    last_pivot = i

        return zigzag

    @staticmethod
    def calculate_aroon(high: pd.Series, low: pd.Series, period: int = 14) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """Calculate Aroon Indicator with proper implementation"""
        def rolling_argmax_position(series, window):
            """Calculate position of maximum in rolling window"""
            result = pd.Series(index=series.index, dtype=float)
            for i in range(window-1, len(series)):
                window_slice = series.iloc[i-window+1:i+1]
                max_idx = window_slice.idxmax()
                # Convert to position within window
                position = (max_idx - window_slice.index[0]).days if hasattr(max_idx - window_slice.index[0], 'days') else (max_idx - window_slice.index[0])
                result.iloc[i] = position
            return result.fillna(0)

        def rolling_argmin_position(series, window):
            """Calculate position of minimum in rolling window"""
            result = pd.Series(index=series.index, dtype=float)
            for i in range(window-1, len(series)):
                window_slice = series.iloc[i-window+1:i+1]
                min_idx = window_slice.idxmin()
                # Convert to position within window
                position = (min_idx - window_slice.index[0]).days if hasattr(min_idx - window_slice.index[0], 'days') else (min_idx - window_slice.index[0])
                result.iloc[i] = position
            return result.fillna(0)

        # Aroon Up
        aroon_up = ((period - rolling_argmax_position(high, period)) / period) * 100

        # Aroon Down
        aroon_down = ((period - rolling_argmin_position(low, period)) / period) * 100

        # Aroon Oscillator
        aroon_oscillator = aroon_up - aroon_down

        return aroon_up, aroon_down, aroon_oscillator

    @staticmethod
    def calculate_volume_profile(price: pd.Series, volume: pd.Series, bins: int = 50) -> Dict:
        """Calculate Volume Profile"""
        # Create price bins
        price_min, price_max = price.min(), price.max()
        price_bins = np.linspace(price_min, price_max, bins)

        # Calculate volume for each price bin
        volume_profile = {}
        for i in range(len(price_bins) - 1):
            bin_start, bin_end = price_bins[i], price_bins[i+1]
            mask = (price >= bin_start) & (price < bin_end)
            volume_profile[f"{bin_start:.4f}-{bin_end:.4f}"] = volume[mask].sum()

        # Find Point of Control (POC) - price level with highest volume
        poc_price = max(volume_profile, key=volume_profile.get)
        poc_volume = volume_profile[poc_price]

        # Find Value Area (70% of total volume)
        total_volume = sum(volume_profile.values())
        target_volume = total_volume * 0.7

        # Sort by volume descending
        sorted_levels = sorted(volume_profile.items(), key=lambda x: x[1], reverse=True)

        value_area_volume = 0
        value_area_levels = []
        for level, vol in sorted_levels:
            value_area_levels.append(level)
            value_area_volume += vol
            if value_area_volume >= target_volume:
                break

        return {
            'volume_profile': volume_profile,
            'poc_price': poc_price,
            'poc_volume': poc_volume,
            'value_area_levels': value_area_levels,
            'value_area_volume': value_area_volume
        }

    @staticmethod
    def calculate_vwap(high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:
        """Calculate Volume Weighted Average Price (VWAP)"""
        typical_price = (high + low + close) / 3
        vwap = pd.Series(index=typical_price.index, dtype=float)

        cumulative_volume = 0
        cumulative_price_volume = 0

        for i in range(len(typical_price)):
            cumulative_volume += volume.iloc[i]
            cumulative_price_volume += typical_price.iloc[i] * volume.iloc[i]
            vwap.iloc[i] = cumulative_price_volume / cumulative_volume if cumulative_volume > 0 else typical_price.iloc[i]

        return vwap

    @staticmethod
    def calculate_money_flow_index(high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series, period: int = 14) -> pd.Series:
        """Calculate Money Flow Index (MFI)"""
        typical_price = (high + low + close) / 3
        raw_money_flow = typical_price * volume

        # Positive and negative money flow
        money_flow_ratio = pd.Series(index=typical_price.index, dtype=float)

        for i in range(1, len(typical_price)):
            if typical_price.iloc[i] > typical_price.iloc[i-1]:
                money_flow_ratio.iloc[i] = raw_money_flow.iloc[i]
            elif typical_price.iloc[i] < typical_price.iloc[i-1]:
                money_flow_ratio.iloc[i] = -raw_money_flow.iloc[i]
            else:
                money_flow_ratio.iloc[i] = 0

        # Calculate MFI
        positive_flow = money_flow_ratio.rolling(window=period, min_periods=1).apply(lambda x: x[x > 0].sum())
        negative_flow = money_flow_ratio.rolling(window=period, min_periods=1).apply(lambda x: abs(x[x < 0]).sum())

        money_ratio = positive_flow / negative_flow
        mfi = 100 - (100 / (1 + money_ratio))

        return mfi

    @staticmethod
    def calculate_ease_of_movement(high: pd.Series, low: pd.Series, volume: pd.Series, period: int = 14) -> pd.Series:
        """Calculate Ease of Movement (EMV)"""
        distance_moved = ((high + low) / 2) - ((high.shift(1) + low.shift(1)) / 2)
        box_ratio = (volume / 100000000) / ((high - low) / ((high + low) / 2))
        emv = distance_moved / box_ratio
        emv_smoothed = emv.rolling(window=period, min_periods=1).mean()

        return emv_smoothed

class ForexDataProcessor:
    """Advanced Forex Data Processing with Comprehensive Indicators"""

    def __init__(self):
        self.lorentzian_classifier = LorentzianClassifier()
        self.indicators = AdvancedTechnicalIndicators()
        self.memory_monitor = MemoryMonitor()

    def load_and_preprocess_data(self, data_path: Union[str, Path]) -> pd.DataFrame:
        """Load and preprocess forex data"""
        logger.info(f"Loading data from {data_path}")

        if isinstance(data_path, str):
            data_path = Path(data_path)

        if data_path.is_file():
            df = pd.read_parquet(data_path) if data_path.suffix == '.parquet' else pd.read_csv(data_path)
        else:
            # Load from directory
            all_files = list(data_path.glob('*.parquet')) + list(data_path.glob('*.csv'))
            dfs = []
            for file in all_files[:5]:  # Limit to first 5 files for processing
                try:
                    if file.suffix == '.parquet':
                        dfs.append(pd.read_parquet(file))
                    else:
                        dfs.append(pd.read_csv(file))
                except Exception as e:
                    logger.warning(f"Failed to load {file}: {e}")
                    continue

            if not dfs:
                raise ValueError("No valid data files found")

            df = pd.concat(dfs, ignore_index=True)

        # Basic preprocessing
        df = df.sort_values('timestamp').drop_duplicates()
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.set_index('timestamp')

        # Ensure required columns exist
        required_cols = ['open', 'high', 'low', 'close']
        if not all(col in df.columns for col in required_cols):
            raise ValueError(f"Missing required columns: {required_cols}")

        # Add volume if missing
        if 'volume' not in df.columns:
            df['volume'] = np.random.randint(1000, 10000, size=len(df))

        logger.info(f"‚úÖ Loaded {len(df)} rows of data")
        return df

    def create_comprehensive_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create comprehensive feature set with all indicators"""
        logger.info("üîß Creating comprehensive technical indicators...")

        # Memory check
        if self.memory_monitor.should_cleanup():
            self.memory_monitor.cleanup()

        # ===== BASIC PRICE FEATURES =====
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))

        # ===== MOVING AVERAGES =====
        for period in [5, 10, 20, 30, 50, 100, 200]:
            df[f'sma_{period}'] = df['close'].rolling(window=period, min_periods=1).mean()
            df[f'ema_{period}'] = df['close'].ewm(span=period, min_periods=1).mean()

        # ===== MOMENTUM INDICATORS =====
        # RSI
        df['rsi_14'] = self.calculate_rsi(df['close'], 14)
        df['rsi_7'] = self.calculate_rsi(df['close'], 7)
        df['rsi_21'] = self.calculate_rsi(df['close'], 21)

        # MACD
        macd, signal, histogram = self.calculate_macd(df['close'])
        df['macd'] = macd
        df['macd_signal'] = signal
        df['macd_histogram'] = histogram

        # Stochastic Oscillator
        stoch_k, stoch_d = self.calculate_stochastic(df['high'], df['low'], df['close'])
        df['stoch_k'] = stoch_k
        df['stoch_d'] = stoch_d

        # Williams %R
        df['williams_r'] = self.calculate_williams_r(df['high'], df['low'], df['close'])

        # Ultimate Oscillator
        df['ultimate_oscillator'] = self.calculate_ultimate_oscillator(df['high'], df['low'], df['close'])

        # ===== VOLATILITY INDICATORS =====
        # Bollinger Bands
        bb_upper, bb_middle, bb_lower = self.calculate_bollinger_bands(df['close'])
        df['bb_upper'] = bb_upper
        df['bb_middle'] = bb_middle
        df['bb_lower'] = bb_lower
        df['bb_width'] = (bb_upper - bb_lower) / bb_middle
        df['bb_position'] = (df['close'] - bb_lower) / (bb_upper - bb_lower)

        # ATR
        df['atr_14'] = self.calculate_atr(df['high'], df['low'], df['close'], 14)

        # Keltner Channels
        keltner_upper, keltner_middle, keltner_lower = self.calculate_keltner_channels(df['high'], df['low'], df['close'])
        df['keltner_upper'] = keltner_upper
        df['keltner_middle'] = keltner_middle
        df['keltner_lower'] = keltner_lower

        # ===== TREND INDICATORS =====
        # ADX
        adx, di_plus, di_minus = self.calculate_adx(df['high'], df['low'], df['close'])
        df['adx'] = adx
        df['di_plus'] = di_plus
        df['di_minus'] = di_minus

        # CCI
        df['cci'] = self.calculate_cci(df['high'], df['low'], df['close'])

        # Ichimoku Cloud
        tenkan, kijun, senkou_a, senkou_b, chikou = self.calculate_ichimoku(df['high'], df['low'], df['close'])
        df['ichimoku_tenkan'] = tenkan
        df['ichimoku_kijun'] = kijun
        df['ichimoku_senkou_a'] = senkou_a
        df['ichimoku_senkou_b'] = senkou_b
        df['ichimoku_chikou'] = chikou

        # Vortex Indicator
        vortex_plus, vortex_minus = self.calculate_vortex_indicator(df['high'], df['low'], df['close'])
        df['vortex_plus'] = vortex_plus
        df['vortex_minus'] = vortex_minus

        # Aroon Oscillator (Fixed)
        aroon_up, aroon_down, aroon_osc = self.indicators.calculate_aroon(df['high'], df['low'])
        df['aroon_up'] = aroon_up
        df['aroon_down'] = aroon_down
        df['aroon_oscillator'] = aroon_osc

        # ===== ADVANCED INDICATORS =====
        # Fractals
        bearish_fractal, bullish_fractal = self.indicators.calculate_fractals(df['high'], df['low'])
        df['bearish_fractal'] = bearish_fractal
        df['bullish_fractal'] = bullish_fractal

        # ZigZag
        df['zigzag'] = self.indicators.calculate_zigzag(df['high'], df['low'])

        # ===== VOLUME INDICATORS =====
        # OBV
        df['obv'] = self.calculate_obv(df['close'], df['volume'])

        # Chaikin Money Flow
        df['cmf'] = self.calculate_chaikin_money_flow(df['high'], df['low'], df['close'], df['volume'])

        # Force Index
        df['force_index'] = self.calculate_force_index(df['close'], df['volume'])

        # VWAP
        df['vwap'] = self.indicators.calculate_vwap(df['high'], df['low'], df['close'], df['volume'])

        # Money Flow Index
        df['mfi'] = self.indicators.calculate_money_flow_index(df['high'], df['low'], df['close'], df['volume'])

        # Ease of Movement
        df['emv'] = self.indicators.calculate_ease_of_movement(df['high'], df['low'], df['volume'])

        # Volume Profile (simplified)
        df['volume_sma_10'] = df['volume'].rolling(window=10, min_periods=1).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma_10']

        # ===== PRICE ACTION FEATURES =====
        # Price momentum
        for period in [1, 3, 5, 10, 15, 20]:
            df[f'momentum_{period}'] = df['close'] / df['close'].shift(period) - 1

        # High-Low range and volatility
        df['range'] = (df['high'] - df['low']) / df['close']
        df['range_sma_10'] = df['range'].rolling(window=10, min_periods=1).mean()

        # ===== LAG FEATURES =====
        for lag in [1, 2, 3, 5, 8, 10, 15]:
            df[f'close_lag_{lag}'] = df['close'].shift(lag)
            df[f'returns_lag_{lag}'] = df['returns'].shift(lag)
            df[f'high_lag_{lag}'] = df['high'].shift(lag)
            df[f'low_lag_{lag}'] = df['low'].shift(lag)

        # ===== DERIVED FEATURES =====
        # Price position relative to moving averages
        df['price_vs_sma20'] = df['close'] / df['sma_20'] - 1
        df['price_vs_sma50'] = df['close'] / df['sma_50'] - 1

        # RSI divergence signals
        df['rsi_divergence'] = df['rsi_14'] - df['rsi_14'].rolling(window=10, min_periods=1).mean()

        # MACD crossover signals
        df['macd_crossover'] = np.where(df['macd'] > df['macd_signal'], 1, -1)

        # ===== TARGET VARIABLE =====
        df['target'] = df['returns'].shift(-1)

        # ===== CLEANUP =====
        # Fill NaN values carefully
        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)

        # Only drop rows where target is NaN (the last row)
        df = df.dropna(subset=['target'])

        logger.info(f"‚úÖ Created {len(df.columns)} features from {len(df)} samples")
        return df

    # Include all the indicator calculation methods from previous implementation
    # (RSI, MACD, Bollinger Bands, Stochastic, Williams %R, CCI, ATR, ADX, OBV, etc.)

    def calculate_rsi(self, price, period=14):
        delta = price.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))

    def calculate_macd(self, price, fast=12, slow=26, signal=9):
        ema_fast = price.ewm(span=fast, min_periods=1).mean()
        ema_slow = price.ewm(span=slow, min_periods=1).mean()
        macd = ema_fast - ema_slow
        signal_line = macd.ewm(span=signal, min_periods=1).mean()
        histogram = macd - signal_line
        return macd, signal_line, histogram

    def calculate_bollinger_bands(self, price, period=20, std_dev=2):
        sma = price.rolling(window=period, min_periods=1).mean()
        std = price.rolling(window=period, min_periods=1).std()
        upper_band = sma + (std * std_dev)
        lower_band = sma - (std * std_dev)
        return upper_band, sma, lower_band

    def calculate_stochastic(self, high, low, close, k_period=14, d_period=3):
        lowest_low = low.rolling(window=k_period, min_periods=1).min()
        highest_high = high.rolling(window=k_period, min_periods=1).max()
        k_percent = 100 * ((close - lowest_low) / (highest_high - lowest_low))
        d_percent = k_percent.rolling(window=d_period, min_periods=1).mean()
        return k_percent, d_percent

    def calculate_williams_r(self, high, low, close, period=14):
        highest_high = high.rolling(window=period, min_periods=1).max()
        lowest_low = low.rolling(window=period, min_periods=1).min()
        williams_r = -100 * ((highest_high - close) / (highest_high - lowest_low))
        return williams_r

    def calculate_cci(self, high, low, close, period=20):
        typical_price = (high + low + close) / 3
        sma_tp = typical_price.rolling(window=period, min_periods=1).mean()
        mad = (typical_price - sma_tp).abs().rolling(window=period, min_periods=1).mean()
        cci = (typical_price - sma_tp) / (0.015 * mad)
        return cci

    def calculate_atr(self, high, low, close, period=14):
        high_low = high - low
        high_close = (high - close.shift(1)).abs()
        low_close = (low - close.shift(1)).abs()
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        atr = true_range.rolling(window=period, min_periods=1).mean()
        return atr

    def calculate_adx(self, high, low, close, period=14):
        tr = self.calculate_atr(high, low, close, 1)
        dm_plus = np.where((high - high.shift(1)) > (low.shift(1) - low),
                          np.maximum(high - high.shift(1), 0), 0)
        dm_minus = np.where((low.shift(1) - low) > (high - high.shift(1)),
                           np.maximum(low.shift(1) - low, 0), 0)
        di_plus = 100 * (pd.Series(dm_plus).rolling(window=period, min_periods=1).mean() / tr)
        di_minus = 100 * (pd.Series(dm_minus).rolling(window=period, min_periods=1).mean() / tr)
        dx = 100 * abs(di_plus - di_minus) / (di_plus + di_minus)
        adx = dx.rolling(window=period, min_periods=1).mean()
        return adx, di_plus, di_minus

    def calculate_obv(self, close, volume):
        obv = pd.Series(index=close.index, dtype=float)
        obv.iloc[0] = volume.iloc[0]
        for i in range(1, len(close)):
            if close.iloc[i] > close.iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] + volume.iloc[i]
            elif close.iloc[i] < close.iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] - volume.iloc[i]
            else:
                obv.iloc[i] = obv.iloc[i-1]
        return obv

    def calculate_ichimoku(self, high, low, close):
        tenkan_sen = (high.rolling(window=9, min_periods=1).max() +
                      low.rolling(window=9, min_periods=1).min()) / 2
        kijun_sen = (high.rolling(window=26, min_periods=1).max() +
                     low.rolling(window=26, min_periods=1).min()) / 2
        senkou_span_a = ((tenkan_sen + kijun_sen) / 2).shift(26)
        senkou_span_b = ((high.rolling(window=52, min_periods=1).max() +
                          low.rolling(window=52, min_periods=1).min()) / 2).shift(26)
        chikou_span = close.shift(-26)
        return tenkan_sen, kijun_sen, senkou_span_a, senkou_span_b, chikou_span

    def calculate_vortex_indicator(self, high, low, close, period=14):
        tr = self.calculate_atr(high, low, close, 1)
        vm_plus = abs(high - low.shift(1))
        vm_minus = abs(low - high.shift(1))
        vi_plus = vm_plus.rolling(window=period, min_periods=1).sum() / tr.rolling(window=period, min_periods=1).sum()
        vi_minus = vm_minus.rolling(window=period, min_periods=1).sum() / tr.rolling(window=period, min_periods=1).sum()
        return vi_plus, vi_minus

    def calculate_chaikin_money_flow(self, high, low, close, volume, period=21):
        money_flow_multiplier = ((close - low) - (high - close)) / (high - low)
        money_flow_volume = money_flow_multiplier * volume
        cmf = money_flow_volume.rolling(window=period, min_periods=1).sum() / volume.rolling(window=period, min_periods=1).sum()
        return cmf

    def calculate_force_index(self, close, volume, period=13):
        force_index = (close.diff() * volume).rolling(window=period, min_periods=1).mean()
        return force_index

    def calculate_ultimate_oscillator(self, high, low, close, period1=7, period2=14, period3=28):
        bp = close - pd.concat([low, close.shift(1)], axis=1).min(axis=1)
        tr = pd.concat([high - low, (high - close.shift(1)).abs(), (low - close.shift(1)).abs()], axis=1).max(axis=1)
        avg1 = bp.rolling(window=period1, min_periods=1).sum() / tr.rolling(window=period1, min_periods=1).sum()
        avg2 = bp.rolling(window=period2, min_periods=1).sum() / tr.rolling(window=period2, min_periods=1).sum()
        avg3 = bp.rolling(window=period3, min_periods=1).sum() / tr.rolling(window=period3, min_periods=1).sum()
        uo = 100 * (4 * avg1 + 2 * avg2 + avg3) / 7
        return uo

    def calculate_keltner_channels(self, high, low, close, period=20, multiplier=2):
        typical_price = (high + low + close) / 3
        middle_line = typical_price.rolling(window=period, min_periods=1).mean()
        atr = self.calculate_atr(high, low, close, period)
        upper_channel = middle_line + (multiplier * atr)
        lower_channel = middle_line - (multiplier * atr)
        return upper_channel, middle_line, lower_channel

class ForexModelTrainer:
    """Advanced Forex Model Training with CatBoost and Lorentzian Classification"""

    def __init__(self):
        self.model = None
        self.scaler = RobustScaler()
        self.feature_names = []
        self.memory_monitor = MemoryMonitor()

    def prepare_features_and_target(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """Prepare features and target for training"""
        # Select feature columns (exclude target and non-feature columns)
        exclude_cols = ['target', 'symbol', 'timestamp']
        feature_cols = [col for col in df.columns if col not in exclude_cols and not col.startswith('returns')]

        X = df[feature_cols]
        y = df['target']

        # Store feature names
        self.feature_names = feature_cols

        logger.info(f"Prepared {len(feature_cols)} features and {len(y)} target values")
        return X, y

    def train_catboost_model(self, X_train: pd.DataFrame, y_train: pd.Series,
                           X_val: pd.DataFrame, y_val: pd.Series) -> CatBoostRegressor:
        """Train CatBoost model with optimized parameters"""
        logger.info("ü§ñ Training CatBoost model...")

        # CatBoost parameters optimized for forex data
        model = CatBoostRegressor(
            iterations=2000,
            learning_rate=0.03,
            depth=8,
            l2_leaf_reg=3,
            border_count=254,
            random_strength=1,
            bagging_temperature=1,
            od_type='Iter',
            od_wait=50,
            verbose=100,
            random_seed=RANDOM_SEED,
            task_type='CPU',
            early_stopping_rounds=100
        )

        # Train the model
        model.fit(
            X_train, y_train,
            eval_set=(X_val, y_val),
            use_best_model=True
        )

        self.model = model
        logger.info("‚úÖ CatBoost model trained successfully")
        return model

    def evaluate_model(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """Evaluate model performance"""
        logger.info("üìä Evaluating model performance...")

        y_pred = self.model.predict(X_test)

        # Calculate metrics
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        # Additional metrics
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        explained_variance = 1 - np.var(y_test - y_pred) / np.var(y_test)

        metrics = {
            'mse': mse,
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'mape': mape,
            'explained_variance': explained_variance,
            'predictions': y_pred,
            'actuals': y_test.values
        }

        logger.info("üìà Model Performance Metrics:")
        logger.info(".6f")
        logger.info(".6f")
        logger.info(".6f")
        logger.info(".6f")
        logger.info(".2f")
        logger.info(".6f")

        return metrics

    def get_feature_importance(self) -> Dict[str, float]:
        """Get feature importance from the trained model"""
        if self.model is None:
            raise ValueError("Model not trained yet")

        importance_values = self.model.get_feature_importance()
        feature_importance = dict(zip(self.feature_names, importance_values))

        # Sort by importance
        sorted_importance = dict(sorted(feature_importance.items(),
                                      key=lambda x: x[1], reverse=True))

        return sorted_importance

    def save_model(self, model_path: Union[str, Path], metrics: Dict):
        """Save trained model and metrics"""
        if self.model is None:
            raise ValueError("No model to save")

        # Save model
        self.model.save_model(str(model_path))
        logger.info(f"üíæ Model saved to {model_path}")

        # Save metrics
        metrics_path = str(model_path).replace('.cbm', '_metrics.json')
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2, default=str)
        logger.info(f"üìä Metrics saved to {metrics_path}")

    def load_model(self, model_path: Union[str, Path]) -> CatBoostRegressor:
        """Load trained model"""
        self.model = CatBoostRegressor()
        self.model.load_model(str(model_path))
        logger.info(f"üìÇ Model loaded from {model_path}")
        return self.model

class ForexVisualizer:
    """Advanced Visualization for Forex Analysis"""

    def __init__(self):
        plt.style.use('default')
        sns.set_palette("husl")

    def plot_price_with_indicators(self, df: pd.DataFrame, title: str = "Price Chart with Indicators"):
        """Plot price chart with technical indicators"""
        fig, axes = plt.subplots(3, 1, figsize=(15, 12), sharex=True)

        # Price chart
        axes[0].plot(df.index, df['close'], label='Close Price', linewidth=1.5)
        axes[0].plot(df.index, df['sma_20'], label='SMA 20', alpha=0.7)
        axes[0].plot(df.index, df['sma_50'], label='SMA 50', alpha=0.7)
        axes[0].fill_between(df.index, df['bb_lower'], df['bb_upper'], alpha=0.2, label='Bollinger Bands')
        axes[0].set_title(f'{title} - Price & Moving Averages')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # RSI
        axes[1].plot(df.index, df['rsi_14'], label='RSI 14', color='purple')
        axes[1].axhline(y=70, color='r', linestyle='--', alpha=0.5, label='Overbought (70)')
        axes[1].axhline(y=30, color='g', linestyle='--', alpha=0.5, label='Oversold (30)')
        axes[1].set_title('RSI Indicator')
        axes[1].set_ylim(0, 100)
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        # MACD
        axes[2].plot(df.index, df['macd'], label='MACD', color='blue')
        axes[2].plot(df.index, df['macd_signal'], label='Signal', color='red')
        axes[2].bar(df.index, df['macd_histogram'], label='Histogram', alpha=0.5, color='gray')
        axes[2].set_title('MACD Indicator')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)

        plt.tight_layout()
        return fig

    def plot_feature_importance(self, feature_importance: Dict[str, float], top_n: int = 20):
        """Plot feature importance"""
        # Get top N features
        sorted_features = dict(sorted(feature_importance.items(),
                                    key=lambda x: x[1], reverse=True)[:top_n])

        fig, ax = plt.subplots(figsize=(12, 8))
        bars = ax.barh(list(sorted_features.keys()), list(sorted_features.values()))
        ax.set_title(f'Top {top_n} Feature Importance')
        ax.set_xlabel('Importance Score')

        # Add value labels on bars
        for bar, value in zip(bars, sorted_features.values()):
            ax.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,
                   '.4f', ha='left', va='center', fontsize=8)

        plt.tight_layout()
        return fig

    def plot_predictions_vs_actuals(self, y_true: np.ndarray, y_pred: np.ndarray):
        """Plot predictions vs actual values"""
        fig, axes = plt.subplots(2, 1, figsize=(15, 10))

        # Scatter plot
        axes[0].scatter(y_true, y_pred, alpha=0.5, s=1)
        axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()],
                    'r--', linewidth=2, label='Perfect Prediction')
        axes[0].set_xlabel('Actual Values')
        axes[0].set_ylabel('Predicted Values')
        axes[0].set_title('Predictions vs Actual Values')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # Residuals plot
        residuals = y_true - y_pred
        axes[1].scatter(y_pred, residuals, alpha=0.5, s=1)
        axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)
        axes[1].set_xlabel('Predicted Values')
        axes[1].set_ylabel('Residuals')
        axes[1].set_title('Residuals Plot')
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        return fig

def main():
    """Main execution function"""
    logger.info("üöÄ Starting Ultimate Forex Training System")
    start_time = datetime.now()

    try:
        # Initialize components
        data_processor = ForexDataProcessor()
        model_trainer = ForexModelTrainer()
        visualizer = ForexVisualizer()

        # Load and preprocess data
        data_path = Path('data/processed')
        df = data_processor.load_and_preprocess_data(data_path)

        # Create comprehensive features
        feature_df = data_processor.create_comprehensive_features(df)

        # Prepare features and target
        X, y = model_trainer.prepare_features_and_target(feature_df)

        # Split data (time-based split for time series)
        split_idx = int(len(X) * 0.7)  # 70% train, 15% validation, 15% test
        val_idx = int(len(X) * 0.85)

        X_train = X.iloc[:split_idx]
        y_train = y.iloc[:split_idx]
        X_val = X.iloc[split_idx:val_idx]
        y_val = y.iloc[split_idx:val_idx]
        X_test = X.iloc[val_idx:]
        y_test = y.iloc[val_idx:]

        logger.info(f"üìä Data split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")

        # Train CatBoost model
        model = model_trainer.train_catboost_model(X_train, y_train, X_val, y_val)

        # Evaluate model
        test_metrics = model_trainer.evaluate_model(X_test, y_test)

        # Get feature importance
        feature_importance = model_trainer.get_feature_importance()

        # Save model and metrics
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = f"models/trained/ultimate_forex_model_{timestamp}.cbm"
        model_trainer.save_model(model_path, test_metrics)

        # Create visualizations
        logger.info("üìä Generating visualizations...")

        # Price chart with indicators (sample)
        sample_df = feature_df.iloc[-500:]  # Last 500 data points
        price_chart = visualizer.plot_price_with_indicators(
            sample_df, "Ultimate Forex Analysis - Sample Data"
        )
        price_chart.savefig(f"visualizations/price_chart_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close(price_chart)

        # Feature importance plot
        importance_chart = visualizer.plot_feature_importance(feature_importance, top_n=25)
        importance_chart.savefig(f"visualizations/feature_importance_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close(importance_chart)

        # Predictions vs actuals plot
        pred_chart = visualizer.plot_predictions_vs_actuals(
            test_metrics['actuals'], test_metrics['predictions']
        )
        pred_chart.savefig(f"visualizations/predictions_analysis_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close(pred_chart)

        # Log completion
        duration = (datetime.now() - start_time).total_seconds()
        logger.info("üéâ Ultimate Forex Training System Completed!")
        logger.info(".2f")
        logger.info(f"üìÅ Model saved: {model_path}")
        logger.info(f"üìä Final R¬≤ Score: {test_metrics['r2']:.6f}")
        logger.info(f"üéØ Total Features: {len(feature_importance)}")
        logger.info(f"üìà Best Feature: {max(feature_importance, key=feature_importance.get)} ({max(feature_importance.values()):.4f})")

        # Summary report
        logger.info("\\n" + "="*60)
        logger.info("üìã ULTIMATE FOREX TRAINING SUMMARY")
        logger.info("="*60)
        logger.info(f"‚úÖ Data Processed: {len(df):,} rows")
        logger.info(f"‚úÖ Features Created: {len(feature_df.columns)}")
        logger.info(f"‚úÖ Model Performance: R¬≤ = {test_metrics['r2']:.6f}")
        logger.info(f"‚úÖ Training Time: {duration:.2f} seconds")
        logger.info(f"‚úÖ Visualizations: 3 charts generated")
        logger.info(f"‚úÖ Production Ready: Model and metrics saved")
        logger.info("="*60)

    except Exception as e:
        logger.error(f"‚ùå Training failed: {str(e)}", exc_info=True)
        raise

if __name__ == "__main__":
    # Create necessary directories
    for dir_path in ['logs', 'models/trained', 'visualizations']:
        Path(dir_path).mkdir(exist_ok=True)

    main()